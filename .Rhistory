library(igraph)
#install.packages("akmeans")
library("akmeans")
## Influence Propagation
# Arguments:
# Graph = Structural Graph, Igraph Instance
# Comm  = Communiity/Clusters
# Beta = Influence Propagation Rate
influence_propagation <- function(graph, comm, beta=0.75){
output = c()
v = vcount(graph)
nodes <- data.frame(1:v)
colnames(nodes) <- c("Id")
nodes$comm = comm
nodes$color = "green"
influencers = c()
for(k in unique(comm)){
influencer = sample(nodes[which(nodes$comm %in% k), c("Id")],1)
influencers = append(influencers, influencer)
}
nodes[nodes$Id %in% influencers, c("color")] <- "red"
new_influencers = influencers
total_influencers = length(influencers)
timestep = 1
old_influencers = 0
ratio = total_influencers/v
cat("Time Step = " , timestep,", Total Influencers = ", total_influencers, ", Network Coverage = " , ratio ,"\n")
output = data.frame(timestep, total_influencers)
timestep = timestep + 1
alpha = 0.7
#while(total_influencers - old_influencers > 0 && ratio < 1){
for(i in 1:15){
old_influencers = total_influencers
influencers = new_influencers
for(i in influencers){
nbs = neighbors(graph, i)
## Selection if the propagation is inside community or outside
flag = "inside"
if(sample(1:100, 1) <= alpha*100){
candidates = nodes[nodes$comm == nodes[i, c("comm")] & nodes$color == "green", c("Id")]
candidates = intersect(candidates, nbs)
ids <- c()
for(j in candidates){
if(j %in% nbs && sample(1:100, 1) <= beta * 100){     # Stochastically determining if node will get infected, using beta transmission probability
ids <- append(ids, j)
}
}
}else{
candidates = nodes[nodes$comm != nodes[i, c("comm")] & nodes$color == "green", c("Id")]
candidates = intersect(candidates, nbs)
ids <- c()
for(j in candidates){
if(sample(1:100, 1) <= beta * 100){     # Stochastically determining if node will get infected, using beta transmission probability
ids <- append(ids, j)
}
}
}
nodes[which(nodes$Id %in% ids), c("color")] <- "red"
total_influencers = nrow(nodes[nodes$color == "red",])
new_influencers = append(new_influencers, unique(ids))
}
ratio = nrow(nodes[nodes$color == "red",])/v
cat("Time Step = " ,timestep,", Total Influencers = ", total_influencers, ", Network Coverage = ", ratio ,"\n")
output = rbind(output, data.frame(timestep, total_influencers))
timestep = timestep + 1
}
output
}
## Influence Propagation Example
# ===== Reading the graph ====== #
g <- read.graph(file="data/fb_caltech_small_edgelist.txt", format = c("edgelist"))
# ===== Read communities file ==== #
comm = integer(vcount(g))
conn <- file("communities.txt", open="r")
lines <-readLines(conn)
for (i in 1:length(lines)){
verts = unlist(strsplit(lines[i], ","))
verts = as.integer(verts) + 1   # R indexes start at 1
comm[verts] = i
}
close(conn)
# ===== Clusters From Adaptive K-means ===== #
attrData <- read.csv("data/fb_caltech_small_attrlist.csv")
akm = akmeans(attrData, d.metric=2, ths3=.4, mode=3,
min.k=length(unique(comm)), max.k=length(unique(comm))) ## cosine distance based
# ===== Influence Propagation Using adaptive k-means clusters ===== #
op1 = influence_propagation(graph=g, comm = akm$cluster)
# ===== Influence Propagation Using SAC-1 communities  ===== #
op2 = influence_propagation(graph=g, comm = comm)
print("here")
## Plot for the timestep comparison
tsteps = as.vector(op1$timestep)
tsteps = append(tsteps, as.vector(op2$timestep))
plot(op1, type="b", col=2, pch=20, ylim = c(0, vcount(g)), xlim = c(1, max(tsteps)), main = "Influence Propagation")
points(op2, type="b", col=3, pch=20)
legend("bottomright",  c("Adaptive-Kmeans","SAC-1"), col=2:3, pch=c(20,20))
t.test(errorc50, errorsvm, paired = TRUE)
# ========================================
# Multiple Hypothesis Testing
# Part 1: K-fold Cross-Validation Paired t-Test
# Part 2: Analysis of Variance (ANOVA) Test
# Part 3: Wilcoxon Signed Rank test
# ========================================
# Load the required R packages
require(C50)
require(kernlab)
require(cvTools)
require(e1071)
require(stats)
readAndProcessData <-
function(filename,
classColumn = NULL,
removeID = NULL) {
data <- read.csv(filename, header = FALSE)
data.df <- data.frame(data)
if (!is.null(classColumn)) {
colnames(data.df)[classColumn] <- "Class"
}
if (!is.null(removeID)) {
data.df <- data.df[-removeID]
}
# Randomize the data and perform 10-fold Cross-Validation
# See ?sample and ?cvFolds
mydata <- data.df[sample(nrow(data.df)), ]
mydata
}
getModel <-
function(modelName,
predictors,
response,
formula,
data) {
model <- NULL
if (modelName == "C5.0") {
model <- C5.0(predictors, as.factor(response))
}
else if (modelName == "SVM") {
model <- ksvm(as.formula(formula), data)
}
else if (modelName == "NaiveBayes") {
model <- naiveBayes(predictors, response)
}
else if (modelName == "Logistic") {
model <- glm(as.formula(formula), data, family = binomial)
} else{
stop("Invalid classifier")
}
model
}
kFoldCrossValidationPercentErrors <-
function(data,
modelName = c("C5.0", "SVM", "NaiveBayes", "Logistic"),
k = 10) {
classColumn <- which(colnames(data) == "Class")
folds <- cvFolds(nrow(data), k)
errors <- vector()
#Use the training set to train classifier algorithm
for (i in 1:k) {
train <- data[folds$subsets[folds$which != i], ]
test <- data[folds$subsets[folds$which == i], ]
model <-
getModel(modelName, train[-classColumn], train[, classColumn], "as.factor(Class)~.", train)
# Make predictions on the test set and calculate the error percentages made by the trained models
prediction <- predict(model, test[-classColumn])
if (modelName == "Logistic") {
prediction <- predict(model, test[-classColumn], type = "response")
prediction <-
ifelse(prediction > 0.5, levels(test[, classColumn])[-classColumn], levels(test[, classColumn])[classColumn])
}
errors <-
append(errors, mean(prediction != test[, classColumn]) * 100)
}
errors
}
# **********************************************
# Part 1: K-fold Cross-Validation Paired t-Test
# *****************************************
# Load the iris data set
irisdata <- readAndProcessData("datasets/Iris_data.txt", 5)
errorc50 <- kFoldCrossValidationPercentErrors(irisdata, "C5.0")
errorsvm <- kFoldCrossValidationPercentErrors(irisdata, "SVM")
# Perform K-fold Cross-Validation Paired t-Test to compare the means of the two error percentages
t.test(errorc50, errorsvm, paired = TRUE)
# *****************************************
# Part 2: Analysis of Variance (ANOVA) Test
# *****************************************
# Load the Breast Cancer data set
breastcancerdata <-
readAndProcessData("datasets/Wisconsin_Breast_Cancer_data.txt", 2, 1)
# Randomize the data and perform 10-fold Cross-Validation
# See ?sample and ?cvFolds
errorc50 <-
kFoldCrossValidationPercentErrors(breastcancerdata, "C5.0")
errorsvm <-
kFoldCrossValidationPercentErrors(breastcancerdata, "SVM")
errornaivebayes <-
kFoldCrossValidationPercentErrors(breastcancerdata, "NaiveBayes")
errorlogistic <-
kFoldCrossValidationPercentErrors(breastcancerdata, "Logistic")
# Compare the performance of the different classifiers using ANOVA test (see ?aov)
classifiers <- rep("C5.0", length(errorc50))
classifiers <- append(classifiers, rep("SVM", length(errorsvm)))
classifiers <-
append(classifiers, rep("NaiveBayes", length(errornaivebayes)))
classifiers <-
append(classifiers, rep("Logistic", length(errorlogistic)))
errors <- c(errorc50, errorsvm, errornaivebayes, errorlogistic)
errors.df <-
data.frame(cbind(errorc50, errorsvm, errornaivebayes, errorlogistic))
aov(errors ~ classifiers, data = errors.df)
# *****************************************
# Part 3: Wilcoxon Signed Rank test
# *****************************************
# Load the following data sets,
# 1. Iris
irisdata <- readAndProcessData("datasets/Iris_data.txt", 5)
# 2. Ecoli
ecolidata <- readAndProcessData("datasets/Ecoli_data.csv", 9, 1)
# 3. Wisconsin Breast Cancer
breastcancerdata <-
readAndProcessData("datasets/Wisconsin_Breast_Cancer_data.txt", 2, 1)
# 4. Glass
glassdata <- readAndProcessData("datasets/Glass_data.txt", 11, 1)
# 5. Yeast
yeastdata <-  readAndProcessData("datasets/Yeast_data.csv", 10, 1)
errors <- matrix(nrow = 5, ncol = 2)
colnames(errors) <- c("C5.0", "SVM")
rownames(errors) <-
c("Iris", "Ecoli", "BreastCancer", "Glass", "Yeast")
errors["Iris", "C5.0"] = mean(kFoldCrossValidationPercentErrors(irisdata, "C5.0"))
errors["Ecoli", "C5.0"] = mean(kFoldCrossValidationPercentErrors(ecolidata, "C5.0"))
errors["BreastCancer", "C5.0"] = mean(kFoldCrossValidationPercentErrors(breastcancerdata, "C5.0"))
errors["Glass", "C5.0"] = mean(kFoldCrossValidationPercentErrors(glassdata, "C5.0"))
errors["Yeast", "C5.0"] = mean(kFoldCrossValidationPercentErrors(yeastdata, "C5.0"))
errors["Iris", "SVM"] = mean(kFoldCrossValidationPercentErrors(irisdata, "SVM"))
errors["Ecoli", "SVM"] = mean(kFoldCrossValidationPercentErrors(ecolidata, "SVM"))
errors["BreastCancer", "SVM"] = mean(kFoldCrossValidationPercentErrors(breastcancerdata, "SVM"))
errors["Glass", "SVM"] = mean(kFoldCrossValidationPercentErrors(glassdata, "SVM"))
errors["Yeast", "SVM"] = mean(kFoldCrossValidationPercentErrors(yeastdata, "SVM"))
# Compare the performance of the different classifiers using Wilcoxon Signed Rank test (see ?wilcox.test)
wilcox.test(errors)
